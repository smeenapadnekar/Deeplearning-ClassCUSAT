{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Motivation: https://github.com/rslim087a/PyTorch-for-Deep-Learning-and-Computer-Vision-Course-All-Codes-\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([transforms.Resize((32,32)),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomRotation(10),\n",
    "                                      transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)),\n",
    "                                      transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.5, ), (0.5, ))\n",
    "                               ])\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((32,32)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, ), (0.5, ))\n",
    "                               ])\n",
    "training_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_train)\n",
    "validation_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(dataset=training_dataset, batch_size=100, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im_convert(tensor):\n",
    "    image = tensor.clone().detach().numpy()\n",
    "    image = image.transpose(1, 2, 0)\n",
    "    image = image * np.array((0.5, 0.5, 0.5)) + np.array((0.5, 0.5, 0.5))\n",
    "    image = image.clip(0, 1)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(training_loader)\n",
    "images, labels = dataiter.next()\n",
    "fig = plt.figure(figsize=(25, 6))\n",
    "\n",
    "for idx in range(20):\n",
    "    ax = fig.add_subplot(2, 10, idx + 1)\n",
    "    plt.imshow(im_convert(images[idx]))\n",
    "    ax.set_title(classes[labels[idx].item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedLeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, 1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, 1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, 1, padding=1)\n",
    "        # How do we get (4*4*64)? - (32+2-3+1), (32/2), (16+2-3+1), (16/2), (8+2-3+1), (8/2)\n",
    "        self.fc1 = nn.Linear(4*4*64, 512)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*64)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModifiedLeNet()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "running_loss_history = []\n",
    "running_correct_history = []\n",
    "val_running_loss_history = []\n",
    "val_running_correct_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "    val_running_loss = 0.0\n",
    "    val_running_corrects = 0.0\n",
    "\n",
    "    for inputs, labels in training_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in validation_loader:\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss = criterion(val_outputs, val_labels)\n",
    "                \n",
    "                _, val_preds = torch.max(val_outputs, 1)\n",
    "                val_running_loss += val_loss.item()\n",
    "                val_running_corrects += torch.sum(val_preds == val_labels.data)\n",
    "        \n",
    "        epoch_loss = running_loss / len(training_loader)\n",
    "        epoch_acc = running_corrects.float() / len(training_loader)\n",
    "        val_epoch_loss = val_running_loss / len(validation_loader)\n",
    "        val_epoch_acc = val_running_corrects.float() / len(validation_loader)\n",
    "\n",
    "        running_loss_history.append(epoch_loss)\n",
    "        running_correct_history.append(epoch_acc)\n",
    "        val_running_loss_history.append(val_epoch_loss)\n",
    "        val_running_correct_history.append(val_epoch_acc)\n",
    "    \n",
    "        print(\"epoch : \", epoch + 1)\n",
    "        print(\"training loss: {:.4f}, training accuracy: {:.4f}\".format(epoch_loss, epoch_acc.item()))\n",
    "        print(\"validation loss: {:.4f}, validation accuracy: {:.4f}\".format(val_epoch_loss, val_epoch_acc.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(running_loss_history, label='Training loss')\n",
    "plt.plot(val_running_loss_history, label='Validation loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(running_correct_history, label='Training accuracy')\n",
    "plt.plot(val_running_correct_history, label='Validation accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "url = 'http://www.edgeofexistence.org/wp-content/uploads/2017/06/Anilany-helenae_KMullin-1000x697.jpg'\n",
    "response = requests.get(url, stream = True)\n",
    "img = Image.open(response.raw)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.ImageOps\n",
    "img = transform(img) \n",
    "plt.imshow(im_convert(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.shape)\n",
    "img = img.unsqueeze(0)\n",
    "print(img.shape)\n",
    "output = model(img)\n",
    "_, pred = torch.max(output, 1)\n",
    "print(classes[pred.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(validation_loader)\n",
    "images_, labels = dataiter.next()\n",
    "output = model(images_)\n",
    "_, preds = torch.max(output, 1)\n",
    "\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "\n",
    "for idx in np.arange(20):\n",
    "  ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n",
    "  plt.imshow(im_convert(images_[idx]))\n",
    "  ax.set_title(\"{} ({})\".format(str(classes[preds[idx].item()]), str(classes[labels[idx].item()])), color=(\"green\" if preds[idx]==labels[idx] else \"red\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytor",
   "language": "python",
   "name": "pytor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
